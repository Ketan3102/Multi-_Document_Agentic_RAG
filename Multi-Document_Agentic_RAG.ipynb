{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Building a Multi-Document Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7a29d",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# these expect to find a .env file                                                                                                                     # the format for that file is (without the comment)                                                                                                                                       #API_KEYNAME=AStringThatIsTheLongAPIKeyFromSomeService                                                                                                                                     \n",
    "def load_env():\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "def get_openai_api_key():\n",
    "    load_env()\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee74bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from typing import List, Optional\n",
    "\n",
    "def get_doc_tools(\n",
    "    file_path: str,\n",
    "    name: str,\n",
    ") -> str:\n",
    "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    def vector_query(\n",
    "        query: str, \n",
    "        page_numbers: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use to answer questions over a given paper.\n",
    "    \n",
    "        Useful if you have specific questions over the paper.\n",
    "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "    \n",
    "        Args:\n",
    "            query (str): the string query to be embedded.\n",
    "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \n",
    "                if we want to perform a vector search\n",
    "                over all pages. Otherwise, filter by the set of specified pages.\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        page_numbers = page_numbers or []\n",
    "        metadata_dicts = [\n",
    "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "        ]\n",
    "        \n",
    "        query_engine = vector_index.as_query_engine(\n",
    "            similarity_top_k=2,\n",
    "            filters=MetadataFilters.from_dicts(\n",
    "                metadata_dicts,\n",
    "                condition=FilterCondition.OR\n",
    "            )\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "        \n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\",\n",
    "        fn=vector_query\n",
    "    )\n",
    "    \n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=f\"summary_tool_{name}\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            f\"Useful for summarization questions related to {name}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return vector_query_tool, summary_tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff58c52",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0af18c01",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the experiments described in the provided context is PG19.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results demonstrate that the proposed methods, such as LongLoRA, S2-Attn, and LoRA+, achieve comparable performance to full attention or full fine-tuning baselines, while being more efficient in terms of computational cost. Different attention patterns were analyzed, showing varying impacts on model performance during fine-tuning. The models also showed improved perplexity scores as the evaluation context length increased and achieved state-of-the-art performance on various benchmarks, showcasing their effectiveness and generalization capabilities.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is PG19. \n",
      "\n",
      "The evaluation results show that the proposed methods in LongLoRA, such as LongLoRA, S2-Attn, and LoRA+, achieve comparable performance to full attention or full fine-tuning baselines, while being more efficient in terms of computational cost. Different attention patterns were analyzed, showing varying impacts on model performance during fine-tuning. The models also demonstrated improved perplexity scores as the evaluation context length increased and achieved state-of-the-art performance on various benchmarks, highlighting their effectiveness and generalization capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by retrieved passages, and reflect on its own generations using special tokens called reflection tokens. This framework improves the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational resources compared to full fine-tuning. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths without compromising performance. LongLoRA is designed to be parameter-efficient, saving trainable parameters and memory costs. An improved version, LongLoRA+, addresses performance gaps by allowing normalization and embedding layers to be trainable during adaptation. Overall, LongLoRA is noted for its ability to efficiently handle longer sequences and reduce computational overhead compared to full fine-tuning.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by retrieved passages, and reflect on its own generations using special tokens called reflection tokens. This framework improves the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational resources compared to full fine-tuning. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths without compromising performance. LongLoRA is designed to be parameter-efficient, saving trainable parameters and memory costs. An improved version, LongLoRA+, addresses performance gaps by allowing normalization and embedding layers to be trainable during adaptation. Overall, LongLoRA is noted for its ability to efficiently handle longer sequences and reduce computational overhead compared to full fine-tuning.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by retrieved passages, and reflect on its own generations using special tokens called reflection tokens. This framework improves the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational resources compared to full fine-tuning. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths without compromising performance. LongLoRA is designed to be parameter-efficient, saving trainable parameters and memory costs. An improved version, LongLoRA+, addresses performance gaps by allowing normalization and embedding layers to be trainable during adaptation. Overall, LongLoRA is noted for its ability to efficiently handle longer sequences and reduce computational overhead compared to full fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "### Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f9addc",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metagpt.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78fc05d0>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78f43a50>],\n",
       " 'longlora.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f5c3d4910>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f5c3ef750>],\n",
       " 'loftq.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78d9b390>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78d9b750>],\n",
       " 'swebench.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f5c4e78d0>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78b87f10>],\n",
       " 'selfrag.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f758a7550>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f758aa190>],\n",
       " 'zipformer.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78d99310>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f758a1010>],\n",
       " 'values.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f5c3fbbd0>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f758b9950>],\n",
       " 'finetune_fair_diffusion.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f7875f0d0>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78ce1290>],\n",
       " 'knowledge_card.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f787d5890>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f77e747d0>],\n",
       " 'metra.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78ccafd0>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78d29550>],\n",
       " 'vr_mcl.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f60908790>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78bdf410>]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_to_tools_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78fc05d0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78f43a50>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f5c3d4910>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f5c3ef750>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78d9b390>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78d9b750>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f5c4e78d0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78b87f10>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f758a7550>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f758aa190>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78d99310>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f758a1010>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f5c3fbbd0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f758b9950>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f7875f0d0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78ce1290>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f787d5890>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f77e747d0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f78ccafd0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78d29550>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7f8f60908790>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78bdf410>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "all_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a16228",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78f43a50>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78d29550>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7f8f78b87f10>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7e6985c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to metagpt', name='summary_tool_metagpt', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"comparison of evaluation dataset in MetaGPT and SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset in MetaGPT involves tasks related to generating text based on prompts across various domains, while SWE-Bench's dataset focuses on software engineering tasks derived from real GitHub issues and pull requests in Python repositories. MetaGPT's dataset includes language understanding, generation, and reasoning tasks, whereas SWE-Bench's dataset comprises challenges like patch generation, reasoning over long contexts, and navigating codebases. The tasks in SWE-Bench require models to interact with execution environments and perform complex reasoning beyond traditional code generation tasks.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "The evaluation dataset in MetaGPT involves tasks related to generating text based on prompts across various domains. On the other hand, SWE-Bench's dataset focuses on software engineering tasks derived from real GitHub issues and pull requests in Python repositories. \n",
      "\n",
      "MetaGPT's dataset includes language understanding, generation, and reasoning tasks, while SWE-Bench's dataset comprises challenges like patch generation, reasoning over long contexts, and navigating codebases. The tasks in SWE-Bench require models to interact with execution environments and perform complex reasoning beyond traditional code generation tasks.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "The evaluation dataset in MetaGPT involves tasks related to generating text based on prompts across various domains. On the other hand, SWE-Bench's dataset focuses on software engineering tasks derived from real GitHub issues and pull requests in Python repositories. \n",
      "\n",
      "MetaGPT's dataset includes language understanding, generation, and reasoning tasks, while SWE-Bench's dataset comprises challenges like patch generation, reasoning over long contexts, and navigating codebases. The tasks in SWE-Bench require models to interact with execution environments and perform complex reasoning beyond traditional code generation tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Approach in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The approach in LongLoRA involves efficiently extending the context windows of pre-trained large language models (LLMs) using shifted sparse attention (S2-Attn) during fine-tuning. This method allows for training models with longer context lengths while maintaining the original attention architecture during inference. By combining low-rank adaptation (LoRA) with S2-Attn, LongLoRA achieves significant computational savings compared to traditional methods, emphasizing the importance of learnable embedding and normalization layers for effective and efficient long context fine-tuning.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Approach in LoftQ\"}\n",
      "=== Function Output ===\n",
      "The approach in LoftQ involves integrating quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This framework provides an advantageous initialization for subsequent LoRA fine-tuning, which helps alleviate quantization discrepancies and significantly improves generalization in downstream tasks. The method alternates between quantization and singular value decomposition (SVD) to optimize the initial values of the quantized backbone and low-rank adapters, ensuring a more effective integration of quantization and fine-tuning processes.\n",
      "=== LLM Response ===\n",
      "The approach in LongLoRA focuses on extending the context windows of pre-trained large language models (LLMs) efficiently using shifted sparse attention (S2-Attn) during fine-tuning. This method enables training models with longer context lengths while preserving the original attention architecture during inference. By combining low-rank adaptation (LoRA) with S2-Attn, LongLoRA achieves significant computational savings compared to traditional methods, emphasizing the importance of learnable embedding and normalization layers for effective and efficient long context fine-tuning.\n",
      "\n",
      "On the other hand, the approach in LoftQ integrates quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This framework provides a beneficial initialization for subsequent LoRA fine-tuning, which helps mitigate quantization discrepancies and enhances generalization in downstream tasks. The method alternates between quantization and singular value decomposition (SVD) to optimize the initial values of the quantized backbone and low-rank adapters, ensuring a more effective integration of quantization and fine-tuning processes.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
